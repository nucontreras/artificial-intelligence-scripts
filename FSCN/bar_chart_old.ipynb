{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nucontreras/artificial-intelligence-scripts/blob/main/FSCN/bar_chart_old.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W64CYScwd13"
      },
      "source": [
        "<b><FONT size=\"8\" color='264653'>Audiographics Project: sonification for the accessibility of quantitative information in graphic format </FONT>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LSbhuvYwd8m"
      },
      "source": [
        "><b><FONT size=\"3\" color='DimGray'> Sonification for the accessibility of quantitative information in graphic format is part of the research line in Technology and Care of the Millennium Institute for Care Research ([MICARE](https://www.micare.cl/)). The objective of this project is to implement a tool for the transformation of quantitative information presented in visual form into audio to facilitate access to visually handicapped people, using sonification techniques and image analysis through artificial intelligence. </FONT>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT4YTrjY5cj8"
      },
      "source": [
        "><b><FONT size=\"3\" color='DimGray'> This file contains the processing, treatment and tests of a neural network used to filter bar chart images. </FONT>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hutlz3dT4NCx"
      },
      "source": [
        "><b><FONT size=\"3\" color='SlateGray'> Code edited from an old code made in an iPre. </FONT>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_hYGDE_wtAH"
      },
      "source": [
        "<b><FONT size=\"5\" color='MidnightBlue '>Connection to Google Drive </FONT>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7OeQ4PzvKQh",
        "outputId": "7b3424e7-f668-4ba3-e218-ecfe8bdfb551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/AudioGraphs\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd 'gdrive/MyDrive/AudioGraphs'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7q5gU9-xX0v"
      },
      "source": [
        "<b><FONT size=\"5\" color='MidnightBlue '>Import of packages </FONT>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PGt3yRfFxXOo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from random import shuffle, seed\n",
        "\n",
        "# Data processing and visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "# AI models\n",
        "import tensorflow as tf\n",
        "from keras.utils import Sequence, plot_model\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose\n",
        "from keras.models import Model, load_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "seed(420)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE02A7t4F6kh"
      },
      "source": [
        "<b><FONT size=\"4\" color='619b8a'>Path to bar chart images dataset  </FONT>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jWf-E9qRF6sX"
      },
      "outputs": [],
      "source": [
        "path_bar_chart_db = 'Codigos y BD anteriores/Barras/models/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN3nrL0wLuH_"
      },
      "source": [
        "<b><FONT size=\"5\" color='MidnightBlue '>FSCN Model </FONT>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with tf.device('/device:GPU:0'):\n",
        "input_layer = Input(shape=(256, 256, 1), name=\"INPUT\")\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\n",
        "x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
        "output_layer = Conv2D(1, (3, 3), padding='same', name=\"OUTPUT\")(x)\n",
        "\n",
        "model = Model(input_layer, output_layer)\n",
        "model.compile(optimizer='adam', loss=\"mse\")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ku4Ijs53dNi",
        "outputId": "313c3fd9-0134-46fe-e9a6-5a44af4ecc4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " INPUT (InputLayer)          [(None, 256, 256, 1)]     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 256, 256, 64)      640       \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 256, 256, 64)     36928     \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 256, 256, 64)      36928     \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 256, 256, 64)     36928     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 256, 256, 64)      36928     \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 256, 256, 64)     36928     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 256, 256, 64)      36928     \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2DT  (None, 256, 256, 64)     36928     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 256, 256, 64)      36928     \n",
            "                                                                 \n",
            " conv2d_transpose_4 (Conv2DT  (None, 256, 256, 64)     36928     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " OUTPUT (Conv2D)             (None, 256, 256, 1)       577       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 333,569\n",
            "Trainable params: 333,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZyNlhEMJNod"
      },
      "source": [
        "<b><FONT size=\"5\" color='MidnightBlue '>Generator </FONT>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pootz8NeJWb5"
      },
      "source": [
        "><b><FONT size=\"3\" color='DimGray'> Creation of the generator to pass the images to the model for training it. </FONT>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, directory, targets_dir, color_mode=\"grayscale\", subdirs=False, batch_size=32, dim=(256, 256), n_channels=1, shuffle=True):\n",
        "        self.x = self.read_directory(directory, subdirs)\n",
        "        self.directory = directory\n",
        "        self.targets_dir = targets_dir\n",
        "        self.len = len(self.x)\n",
        "        self.batch_size = batch_size\n",
        "        self.dim = dim\n",
        "        self.n_channels = n_channels\n",
        "        self.shuffle = shuffle\n",
        "#         self.on_epoch_end()\n",
        "\n",
        "    def read_directory(self, directory, subdirs):\n",
        "        if not subdirs:\n",
        "            return os.listdir(directory)\n",
        "        raise FileExistsError\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(self.len // self.batch_size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch = self.x[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "        x, y = self.read_images(batch)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle == True:\n",
        "            shuffle(self.x)\n",
        "\n",
        "    def read_images(self, batch):\n",
        "        # Inicialización de datos\n",
        "        # X e y son de igual dimensión ya que ambos son imágenes\n",
        "        # X : (n_samples, *dim, n_channels)\n",
        "        x, y = [], []\n",
        "\n",
        "        # Generación de data\n",
        "        for i, name in enumerate(batch):\n",
        "            path = self.directory + \"/\" + name\n",
        "            img = tf.keras.utils.load_img(path, target_size=(256, 256), color_mode=\"grayscale\")\n",
        "            img = tf.keras.utils.img_to_array(img)\n",
        "            x.append(img / 255)\n",
        "\n",
        "            target_path = self.targets_dir + \"/\" + name\n",
        "            target_img = tf.keras.utils.load_img(target_path, target_size=(256, 256), color_mode=\"grayscale\")\n",
        "            target_img = tf.keras.utils.img_to_array(target_img)\n",
        "            y.append(target_img / 255)\n",
        "\n",
        "        return np.array(x), np.array(y)"
      ],
      "metadata": {
        "id": "rb_p-m3g3d3Z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path_x = path_bar_chart_db + \"dataset/segmentation-2/train/grises\"\n",
        "train_path_y = path_bar_chart_db + \"dataset/segmentation-2/train/segmented\"\n",
        "\n",
        "test_path_x = path_bar_chart_db + \"dataset/segmentation-2/test/grises\"\n",
        "test_path_y = path_bar_chart_db + \"dataset/segmentation-2/test/segmented\"\n",
        "\n",
        "train_gen = DataGenerator(train_path_x, train_path_y)\n",
        "test_gen = DataGenerator(test_path_x, test_path_y)"
      ],
      "metadata": {
        "id": "s4u1YGSW4A5V"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen.__getitem__(3)[1][0].shape\n",
        "# len(train_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAPtuK664XRs",
        "outputId": "d5fe6208-f51f-4ea9-e57a-edd15f08af01"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 256, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp67lvEEL3b7"
      },
      "source": [
        "<b><FONT size=\"5\" color='MidnightBlue '>Training </FONT>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Early Stopping\n",
        "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=5, restore_best_weights=True)\n",
        "\n",
        "#with tf.device('/device:GPU:0'):\n",
        "history = model.fit(train_gen,\n",
        "                    verbose=1,\n",
        "                    steps_per_epoch=5,\n",
        "                    epochs=30)\n",
        "\n",
        "\n",
        "loss = history.history['loss']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZYpeo8wI3ebC",
        "outputId": "831e78e0-1b4f-489f-b46a-b326083e787d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "5/5 [==============================] - 147s 24s/step - loss: 0.5831\n",
            "Epoch 2/30\n",
            "5/5 [==============================] - 138s 27s/step - loss: 0.4548\n",
            "Epoch 3/30\n",
            "5/5 [==============================] - 132s 26s/step - loss: 0.2365\n",
            "Epoch 4/30\n",
            "5/5 [==============================] - 125s 24s/step - loss: 0.1412\n",
            "Epoch 5/30\n",
            "5/5 [==============================] - 123s 24s/step - loss: 0.0979\n",
            "Epoch 6/30\n",
            "5/5 [==============================] - 118s 23s/step - loss: 0.0897\n",
            "Epoch 7/30\n",
            "5/5 [==============================] - 112s 22s/step - loss: 0.0744\n",
            "Epoch 8/30\n",
            "5/5 [==============================] - 110s 22s/step - loss: 0.0651\n",
            "Epoch 9/30\n",
            "5/5 [==============================] - 100s 19s/step - loss: 0.0564\n",
            "Epoch 10/30\n",
            "5/5 [==============================] - 92s 18s/step - loss: 0.0557\n",
            "Epoch 11/30\n",
            "5/5 [==============================] - 92s 19s/step - loss: 0.0565\n",
            "Epoch 12/30\n",
            "5/5 [==============================] - 93s 18s/step - loss: 0.0418\n",
            "Epoch 13/30\n",
            "5/5 [==============================] - 91s 18s/step - loss: 0.0332\n",
            "Epoch 14/30\n",
            "5/5 [==============================] - 86s 18s/step - loss: 0.0240\n",
            "Epoch 15/30\n",
            "5/5 [==============================] - 70s 13s/step - loss: 0.0229\n",
            "Epoch 16/30\n",
            "5/5 [==============================] - 84s 16s/step - loss: 0.0417\n",
            "Epoch 17/30\n",
            "5/5 [==============================] - 75s 15s/step - loss: 0.0284\n",
            "Epoch 18/30\n",
            "5/5 [==============================] - 80s 16s/step - loss: 0.0188\n",
            "Epoch 19/30\n",
            "1/5 [=====>........................] - ETA: 1:04 - loss: 0.0201"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7e1e1259bdee>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#with tf.device('/device:GPU:0'):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m history = model.fit(train_gen,\n\u001b[0m\u001b[1;32m      8\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) UNKNOWN:  FileNotFoundError: [Errno 2] No such file or directory: 'Codigos y BD anteriores/Barras/models/dataset/segmentation-2/train/segmented/3258(1).png'\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 267, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\", line 902, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\", line 1049, in generator_fn\n    yield x[i]\n\n  File \"<ipython-input-11-400dfd8e7754>\", line 23, in __getitem__\n    x, y = self.read_images(batch)\n\n  File \"<ipython-input-11-400dfd8e7754>\", line 45, in read_images\n    target_img = tf.keras.utils.load_img(target_path, target_size=(256, 256), color_mode = \"grayscale\")\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/image_utils.py\", line 422, in load_img\n    with open(path, \"rb\") as f:\n\nFileNotFoundError: [Errno 2] No such file or directory: 'Codigos y BD anteriores/Barras/models/dataset/segmentation-2/train/segmented/3258(1).png'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_7]]\n  (1) UNKNOWN:  FileNotFoundError: [Errno 2] No such file or directory: 'Codigos y BD anteriores/Barras/models/dataset/segmentation-2/train/segmented/3258(1).png'\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 267, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\", line 902, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\", line 1049, in generator_fn\n    yield x[i]\n\n  File \"<ipython-input-11-400dfd8e7754>\", line 23, in __getitem__\n    x, y = self.read_images(batch)\n\n  File \"<ipython-input-11-400dfd8e7754>\", line 45, in read_images\n    target_img = tf.keras.utils.load_img(target_path, target_size=(256, 256), color_mode = \"grayscale\")\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/image_utils.py\", line 422, in load_img\n    with open(path, \"rb\") as f:\n\nFileNotFoundError: [Errno 2] No such file or directory: 'Codigos y BD anteriores/Barras/models/dataset/segmentation-2/train/segmented/3258(1).png'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_2694]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wRuwizSME0O"
      },
      "source": [
        "<b><FONT size=\"5\" color='MidnightBlue '>Evaluation </FONT>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, img_path):\n",
        "    \"Recibe un modelo y un path a una imagen y retorna la imagen y el output como np arrays\"\n",
        "    img = tf.keras.utils.load_img(img_path, target_size=(256, 256), color_mode = \"grayscale\")\n",
        "    img = tf.keras.utils.img_to_array(img) / 255\n",
        "#     print(img.shape)\n",
        "    segmented = model.predict(img)\n",
        "    return img, segmented[:,:,:,0]\n",
        "\n",
        "def normalize_img(img):\n",
        "    return (img - img.min()) / (img.max() - img.min())"
      ],
      "metadata": {
        "id": "T4StiN6H3fAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3\n",
        "img_path = f\"models/dataset/segmentation-2/train/grises/{n}.png\"\n",
        "target_path = f\"models/dataset/segmentation-2/train/segmented/{n}.png\"\n",
        "\n",
        "img, segmented = evaluate(model, img_path)\n",
        "\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
        "fig.set_size_inches(15, 15)\n",
        "\n",
        "ax1.imshow(img, cmap=\"gray\")\n",
        "ax2.imshow(segmented, cmap=\"gray\")\n",
        "\n",
        "target = tf.keras.utils.load_img(target_path, target_size=(256, 256), color_mode = \"grayscale\")\n",
        "target = tf.keras.utils.img_to_array(target)\n",
        "ax3.imshow(target, cmap=\"gray\")\n",
        "\n",
        "# print(segmented)\n",
        "ax4.imshow(normalize_img(segmented), cmap=\"gray\")\n",
        "\n",
        "# print(normalize_img(segmented))\n",
        "# print(segmented)\n",
        "\n",
        "# print(segmented[150], img[150])\n",
        "\n",
        "# print(segmented[:, :, :, 0])"
      ],
      "metadata": {
        "id": "223IC6X046gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><FONT size='5' color='MidnightBlue' >Save Model</FONT></b>"
      ],
      "metadata": {
        "id": "Us0I6Y7gqHsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, name=None):\n",
        "    \"guarda un modelo como h5, si no tiene nombre se guarda con un timestamp\"\n",
        "    path = \"Trained Models/\"\n",
        "    if name == None:\n",
        "        td = datetime.now()\n",
        "        name = f\"{td.year}-{td.year}-{td.day}_{td.hour}-{td.minute}-{td.second}\"\n",
        "    model.save(path + name + \".h5\", save_format=\"h5\")\n",
        "\n",
        "def load_model(model_path):\n",
        "    return tf.keras.models.load_model(model_path)"
      ],
      "metadata": {
        "id": "d4E9vR5_3fkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_data(model_path, img_path):\n",
        "    model = load_model(model_path) # CARGADO DEL MODELO\n",
        "    # IMG EN NP ARRAY (1, 256, 256, 1)\n",
        "    full_img = []\n",
        "    img = tf.keras.utils.load_img(img_path, target_size=(256, 256), color_mode = \"grayscale\")\n",
        "    img = tf.keras.utils.img_to_array(img)\n",
        "    img = ((img - img.min())/(img.max() - img.min())) #MIN MAX\n",
        "    full_img.append(img)\n",
        "    full_img = np.array(full_img)\n",
        "    # PREDICCION\n",
        "    pred = model.predict(full_img)\n",
        "    # NORMALIZACIÓN\n",
        "    pred = ((pred - pred.min())/(pred.max() - pred.min()))\n",
        "\n",
        "    # THRESHOLD\n",
        "    thresh = cv2.threshold(pred[0,:,:,0], 0.8, 1, cv2.THRESH_BINARY)\n",
        "    thresh = thresh[1].reshape(256,256)\n",
        "\n",
        "    # ERODE-DILATE\n",
        "    kernel = np.ones((3,3),np.uint8)\n",
        "    erosion = cv2.erode(thresh,kernel,iterations = 1)\n",
        "    dilate = cv2.dilate(erosion, kernel, iterations = 1)\n",
        "\n",
        "    #DISPLAY\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    plt.gray()\n",
        "    ax = plt.subplot(2,2,1)\n",
        "    plt.imshow(full_img.reshape(256,256))\n",
        "    ax = plt.subplot(2,2,2)\n",
        "    plt.imshow(pred.reshape(256,256))\n",
        "    ax = plt.subplot(2,2,3)\n",
        "    plt.imshow(thresh.reshape(256,256))\n",
        "    ax = plt.subplot(2,2,4)\n",
        "    plt.imshow(dilate.reshape(256,256))"
      ],
      "metadata": {
        "id": "765D8AFT5QSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"Trained Models/2021-2021-18_15-28-38.h5\"\n",
        "n = 3\n",
        "img_path = f\"models/dataset/segmentation-2/test/grises/{n}.png\"\n",
        "target_path = f\"models/dataset/segmentation-2/train/segmented/{n}.png\"\n",
        "\n",
        "show_data(model_path, img_path)"
      ],
      "metadata": {
        "id": "LdGiDOCD5UkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>"
      ],
      "metadata": {
        "id": "z0Hx2t3n3gDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><FONT size='5' color='MidnightBlue' >Load model </FONT></b>"
      ],
      "metadata": {
        "id": "0spWGR09pFwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = load_model('Trained Models/2021-2021-18_15-28-38.h5')\n",
        "m.summary()"
      ],
      "metadata": {
        "id": "fMZXc7a5pHxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(m)\n",
        "m.losses"
      ],
      "metadata": {
        "id": "d4r9LPTBpKIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(m)"
      ],
      "metadata": {
        "id": "pvDvmfAipKOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def minmax(img):\n",
        "  img = (img - img.min())/(img.max() - img.min()) #MIN MAX\n",
        "  return img\n",
        "\n",
        "def test_model(model, img_path, img_shape, target_path=None):\n",
        "  # IMG EN NP ARRAY (1, 256, 256, 1)\n",
        "  # full_img = []\n",
        "  img = tf.keras.utils.load_img(img_path, target_size=(256,256), color_mode = \"grayscale\") #, interpolation=\"bicubic\")\n",
        "  img = tf.keras.utils.img_to_array(img)\n",
        "  # img = minmax(img)\n",
        "\n",
        "  # PREDICCION\n",
        "  pred = model.predict(img[None,:,:,:]).reshape((256, 256))\n",
        "  img = img.reshape((256, 256))\n",
        "\n",
        "  # NORMALIZACIÓN\n",
        "  # pred = ((pred - pred.min())/(pred.max() - pred.min()))\n",
        "  # print(pred[0].shape)\n",
        "  # print(minmax(pred))\n",
        "  ret, thresh = cv2.threshold(minmax(pred), 0.5, 1, cv2.THRESH_BINARY)\n",
        "\n",
        "  #DISPLAY\n",
        "  plt.figure(figsize=(15, 15))\n",
        "  plt.gray()\n",
        "  ax = plt.subplot(2,2,1)\n",
        "  plt.imshow(img)\n",
        "  ax = plt.subplot(2,2,2)\n",
        "  plt.imshow(pred)\n",
        "  ax = plt.subplot(2,2,3)\n",
        "  plt.imshow(thresh)\n",
        "\n",
        "  # ax = plt.subplot(2,2,3)\n",
        "  # plt.imshow(1-new)\n",
        "  # ax = plt.subplot(2,2,4)\n",
        "  # plt.imshow(onepixel_img(new))\n",
        "  if target_path:\n",
        "    target = tf.keras.utils.load_img(target_path, target_size=(256,256), color_mode = \"grayscale\") #, interpolation=\"bicubic\")\n",
        "    target = tf.keras.utils.img_to_array(target)\n",
        "    ax = plt.subplot(2,2,4)\n",
        "    plt.imshow(target[:,:,0])"
      ],
      "metadata": {
        "id": "e-lMCa2PpKRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_path = None\n",
        "n = 1\n",
        "# img_path = drive_path + f\"dataset/segmentation-2/test/color/{n}.png\"\n",
        "img_path = drive_path + f\"ejemplos/internet/buenos/1000.jpg\"\n",
        "# target_path = drive_path + f\"dataset/segmentation-2/test/segmented/{n}.png\"\n",
        "\n",
        "# img_path = drive_path + \"DB graficos internet/Barras Verticales/Copia de graph_1003.jpg\"\n",
        "m = load_model(model_path + '2021-2021-18_15-28-38.h5')\n",
        "\n",
        "test_model(m, img_path, (256, 256), target_path)"
      ],
      "metadata": {
        "id": "0PtaZw60pKUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def segment(img_path, model):\n",
        "  img = tf.keras.utils.load_img(img_path, target_size=(256,256), color_mode = \"grayscale\") #, interpolation=\"bicubic\")\n",
        "  img = tf.keras.utils.img_to_array(img)\n",
        "\n",
        "  # PREDICCION\n",
        "  pred = model.predict(img[None,:,:,:]).reshape((256, 256))\n",
        "  img = img.reshape((256, 256))\n",
        "\n",
        "  ret, thresh = cv2.threshold(minmax(pred), 0.5, 1, cv2.THRESH_BINARY)\n",
        "  return thresh.astype(np.uint8)\n",
        "\n",
        "  # m = load_model(model_path + models[0])"
      ],
      "metadata": {
        "id": "ius43eCkpKXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1016\n",
        "img_path = drive_path + f\"ejemplos/internet/buenos/{n}.jpg\"\n",
        "target_path = drive_path + f\"dataset/segmentation-2/test/segmented/{n}.png\"\n",
        "\n",
        "img_path = drive_path + \"models/dataset/DB graficos internet/Barras Verticales/Copia de graph_1003.jpg\"\n",
        "\n",
        "# print(segmented.astype(np.uint8).dtype)\n",
        "\n",
        "segmented = segment(img_path, m)\n",
        "\n",
        "# Invert de blanco con negro para encontrar los contours\n",
        "contours, hierarchy = cv2.findContours(np.where((segmented==0), 1, 0).astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.imshow(segmented)\n",
        "\n",
        "img = cv2.drawContours(cv2.cvtColor(segmented * 255, cv2.COLOR_GRAY2RGB), contours, -1, (0,255,0), 1)\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.imshow(img)\n",
        "\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.imshow(cv2.drawContours(np.ones((256, 256, 3)) * 255, contours, -1, (0,255,0), 2))\n",
        "\n",
        "contours = [c for c in contours if c.shape[0] > 3]\n",
        "print(len(contours))\n",
        "# print(hierarchy)\n",
        "\n",
        "img = np.ones((256, 256, 3)) * 255\n",
        "for bar in contours:\n",
        "  x_min = bar[:,0,0].min()\n",
        "  x_max = bar[:,0,0].max()\n",
        "  y_min = bar[:,0,1].min()\n",
        "  y_max = bar[:,0,1].max()\n",
        "\n",
        "  cv2.rectangle(img,(x_min, y_min), (x_max, y_max), (0,255,0), 1)\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.imshow(img)\n"
      ],
      "metadata": {
        "id": "ZGxhPXgEpKar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_and_save():\n",
        "    pass\n",
        "\n",
        "def test_and_save_image(model, img_path, img_name, save_dir, display=True):\n",
        "\n",
        "    img = tf.keras.utils.load_img(img_path, target_size=(256,256), color_mode = \"grayscale\") #, interpolation=\"bicubic\")\n",
        "    img = tf.keras.utils.img_to_array(img)\n",
        "\n",
        "    # PREDICCION\n",
        "    pred = model.predict(img[None,:,:,:]).reshape((256, 256))\n",
        "    img = img.reshape((256, 256))\n",
        "\n",
        "    # NORMALIZACIÓN\n",
        "    ret, thresh = cv2.threshold(minmax(pred), 0.5, 1, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Contornos\n",
        "    # Invert de blanco con negro para encontrar los contours\n",
        "    contours, hierarchy = cv2.findContours(np.where((thresh==0), 1, 0).astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    contours = [c for c in contours if c.shape[0] > 3]\n",
        "    # print(len(contours))\n",
        "\n",
        "    contour_img = np.ones((256, 256, 3)) * 255\n",
        "    final_img = tf.keras.utils.load_img(img_path, target_size=(256,256))\n",
        "    final_img = tf.keras.utils.img_to_array(final_img).astype(np.uint8)\n",
        "    # print(final_img.dtype)\n",
        "    for bar in contours:\n",
        "        x_min = bar[:,0,0].min()\n",
        "        x_max = bar[:,0,0].max()\n",
        "        y_min = bar[:,0,1].min()\n",
        "        y_max = bar[:,0,1].max()\n",
        "\n",
        "    cv2.rectangle(contour_img,(x_min, y_min), (x_max, y_max), (0,255,0), 1)\n",
        "    cv2.rectangle(final_img,(x_min, y_min), (x_max, y_max), (0,255,0), 2)\n",
        "\n",
        "    # DISPLAY\n",
        "    if display:\n",
        "        plt.figure(figsize=(15, 15))\n",
        "        plt.gray()\n",
        "        plt.subplot(2,3,1)\n",
        "        plt.imshow(img)\n",
        "        plt.subplot(2,3,2)\n",
        "        plt.imshow(pred)\n",
        "        plt.subplot(2,3,3)\n",
        "        plt.imshow(thresh)\n",
        "        plt.subplot(2, 3, 4)\n",
        "        plt.imshow(contour_img)\n",
        "        plt.subplot(2, 3, 5)\n",
        "        plt.imshow(final_img)\n",
        "\n",
        "    # Save\n",
        "    plt.imsave(save_dir + f\"1_grayscale_reshape/{img_name}\", img)\n",
        "    plt.imsave(save_dir + f\"2_output_red/{img_name}\", pred)\n",
        "    plt.imsave(save_dir + f\"3_threshold/{img_name}\", thresh)\n",
        "    plt.imsave(save_dir + f\"4_contornos/{img_name}\", contour_img / 255)\n",
        "    plt.imsave(save_dir + f\"5_contornos_sobre_original/{img_name}\", final_img)\n"
      ],
      "metadata": {
        "id": "qhpVK7TwpcK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# img_path = drive_path + f\"dataset/segmentation-2/test/segmented/{n}.png\"\n",
        "# test_and_save_image(m, img_path)\n",
        "imgs_path = drive_path + \"models/dataset/segmentation-2/test/color\"\n",
        "\n",
        "for img_name in os.listdir(imgs_path):\n",
        "  img_path = drive_path + f\"models/dataset/segmentation-2/test/color/{img_name}\"\n",
        "  save_dir = drive_path + \"models/dataset/segmentation-2/test/output/\"\n",
        "  test_and_save_image(m, img_path, img_name, save_dir, display=False)\n"
      ],
      "metadata": {
        "id": "Fign06x4pcOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs_path = drive_path + \"models/dataset/internet/0_originales/\"\n",
        "\n",
        "for img_name in os.listdir(imgs_path):\n",
        "    img_path = imgs_path + img_name\n",
        "    save_dir = drive_path + \"models/dataset/internet/\"\n",
        "    test_and_save_image(m, img_path, img_name, save_dir, display=False)"
      ],
      "metadata": {
        "id": "bv9gQx2PpcQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I9bEjR-4pcTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aKnr_WE8pKcp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}